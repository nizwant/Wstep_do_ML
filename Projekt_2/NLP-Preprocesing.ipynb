{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38a19f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import enchant\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')      \n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bae3d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "# ścieżka do plików\n",
    "p = os.path.join(os.getcwd(), 'data')\n",
    "for folder in os.listdir(p):\n",
    "    path = os.path.join(p, folder)\n",
    "    # jeśli plik to pomiń\n",
    "    if os.path.isfile(path):\n",
    "        continue\n",
    "    # jesli folder to pobierz pliki\n",
    "    mails = os.listdir(path)\n",
    "    for mail in mails:\n",
    "        path = os.path.join(p, folder, mail)\n",
    "        with open(path,\"r\") as file:\n",
    "            text = file.read()\n",
    "        data.append([text, folder])\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbc52d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f530af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Preprocesing(data, lematize = False, stem = True):\n",
    "    \n",
    "    # Bierzemy wszystko po pierwszych 2 enterach czyli po opisie technicznym\n",
    "    formated = data.split('\\n\\n')[1:]\n",
    "    formated = ' '.join(formated).lower() # zamiana na małe litery\n",
    "    \n",
    "    # Wyrzucamy wszystkie wyrażenia słowo.słowo(ścieżki itp.) i adresy mail \n",
    "    formated = re.sub('((\\w+\\.)+\\w+)|\\w+@\\w+','', formated)\n",
    "    \n",
    "    # Podmieniamy wszystkie liczby, ciągi liczb na spacje  i \"_\"\n",
    "    formated =re.sub('\\W|\\d|_',\" \", formated)\n",
    "    \n",
    "    # Tokenizacja\n",
    "    tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "    formated= tokenizer.tokenize(formated)\n",
    "    \n",
    "    # Usuwamy stop wordsy\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    formated = [word for word in formated if not word in stop_words and len(word) > 1]\n",
    "    \n",
    "    # Lematyzacja\n",
    "    if(lematize == True):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        formated = [lemmatizer.lemmatize(word) for word in formated] \n",
    "    \n",
    "    # Stemming    \n",
    "    if (stem == True):\n",
    "        ps = PorterStemmer() \n",
    "        formated = [ps.stem(word) for word in formated] \n",
    "        \n",
    "    return formated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3180ff45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[0],\n",
    "    df[1],\n",
    "    test_size=0.4, random_state=213)\n",
    "\n",
    "# Preprocesing\n",
    "X_train = [Preprocesing(text) for text in X_train]\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43531219",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# max i min trzeba zhiperparametryzować wyrzucilismy slowa ktore istnieja w mniej niz 1 procencie tekstow i wiecej niz 0.2\n",
    "\n",
    "tf_idf = [\" \".join(text) for text in X_train]\n",
    "vectorizer = TfidfVectorizer(strip_accents='unicode', stop_words='english', max_df = 0.2 ,min_df=0.01)\n",
    "tf_idf = vectorizer.fit_transform(tf_idf)\n",
    "tf_idf = pd.DataFrame(tf_idf.toarray(), columns = vectorizer.get_feature_names())\n",
    "tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a210cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "clustering = DBSCAN().fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e3d2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(14,8)).clf()\n",
    "sns.histplot(clustering.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fb3f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters =  5, random_state = 0)\n",
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fc87c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,8)).clf()\n",
    "sns.histplot(kmeans.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1d13f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu jeszcze kombinuje bo wcześniej głupot narobiłem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85413bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pewnie do śmieci\n",
    "def modeling(df, number_of_topics):\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    \n",
    "    #matrix = CountVectorizer()\n",
    "    #X = matrix.fit_transform(df).toarray()\n",
    "    #b_of_words = pd.DataFrame(X, columns = matrix.get_feature_names_out())\n",
    "    #tfidf = TfidfVectorizer(lowercase=True,\n",
    "     #                   stop_words='english',\n",
    "    #                    ngram_range = (1,1))\n",
    "\n",
    "    # Fit and Transform the documents\n",
    "    train_data = tfidf.fit_transform(df)  \n",
    "    lsa = TruncatedSVD(n_components=number_of_topics, random_state=42)\n",
    "\n",
    "    # Fit SVD model on data\n",
    "    lsa.fit_transform(train_data)\n",
    "    #b_of_words = df.columns\n",
    "    #terms = tfidf.get_feature_names_out()\n",
    "\n",
    "    for index, component in enumerate(lsa.components_):\n",
    "        zipped = zip(terms, component)\n",
    "        top_terms_key=sorted(zipped, key = lambda t: t[1], reverse=True)[:10]\n",
    "        top_terms_list=list(dict(top_terms_key).keys())\n",
    "        print(\"Topic \"+str(index)+\": \",top_terms_list)\n",
    "        \n",
    "    m1= np.array(lsa.components_.T)\n",
    "    m2= np.array(b_of_words)\n",
    "    X = np.dot(m2,m1)\n",
    "    X = pd.DataFrame(X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8fe0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = modeling(tf_idf, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f273f408",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8eba16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
