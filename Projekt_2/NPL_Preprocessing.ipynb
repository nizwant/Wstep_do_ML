{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a393184e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install gensim\n",
    "# ! pip install np\n",
    "# ! pip install nltp\n",
    "# ! pip install enchant\n",
    "# ! pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38a19f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import enchant\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')      \n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bae3d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "# ścieżka do plików\n",
    "p = os.path.join(os.getcwd(), 'data')\n",
    "for folder in os.listdir(p):\n",
    "    path = os.path.join(p, folder)\n",
    "    # jeśli plik to pomiń\n",
    "    if os.path.isfile(path):\n",
    "        continue\n",
    "    # jesli folder to pobierz pliki\n",
    "    mails = os.listdir(path)\n",
    "    for mail in mails:\n",
    "        path = os.path.join(p, folder, mail)\n",
    "        with open(path, encoding=\"latin-1\") as file:\n",
    "            text = file.read()\n",
    "        data.append([text, folder])\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbc52d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861897ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "# używaliśmy tego w różnych wersjach preprocesingu ostatecznie porzuciliśmy\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].lower()\n",
    "    tag_dict = {\"a\": wordnet.ADJ,\n",
    "                \"n\": wordnet.NOUN,\n",
    "                \"v\": wordnet.VERB,\n",
    "                \"r\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f530af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Preprocesing(data, lematize = False, stem = True, eng = False):\n",
    "    # Najlepsze wyniki były dla samego stemmingu więc jest ustawiony domyślnie na True reszta False\n",
    "    \n",
    "    # Bierzemy wszystko po pierwszych 2 enterach czyli po opisie technicznym\n",
    "    formated = data.split('\\n\\n')[1:]\n",
    "    formated = ' '.join(formated).lower() # zamiana na małe litery\n",
    "    \n",
    "    # Wyrzucamy wszystkie wyrażenia słowo.słowo(ścieżki itp.) i adresy mail \n",
    "    formated = re.sub('((\\w+\\.)+\\w+)|\\w+@\\w+','', formated)\n",
    "    \n",
    "    # Podmieniamy wszystkie liczby, ciągi liczb na spacje  i \"_\"\n",
    "    formated =re.sub('\\W|\\d|_',\" \", formated)\n",
    "    \n",
    "    # Tokenizacja\n",
    "    tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "    formated= tokenizer.tokenize(formated)\n",
    "    \n",
    "    # Usuwamy stop wordsy\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    formated = [word for word in formated if (not word in stop_words) and len(word) > 1]\n",
    "    \n",
    "    # Lematyzacja\n",
    "    if(lematize == True):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        formated = [lemmatizer.lemmatize(word, pos = get_wordnet_pos(word)) for word in formated] \n",
    "    \n",
    "    # Stemming    \n",
    "    if (stem == True):\n",
    "        ps = PorterStemmer() \n",
    "        formated = [ps.stem(word) for word in formated] \n",
    "    \n",
    "    if (eng == True):\n",
    "        d = enchant.Dict(\"en_US\")\n",
    "        formated = [word for word in formated if d.check(word)]\n",
    "        \n",
    "    return formated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3180ff45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[0],\n",
    "    df[1],\n",
    "    test_size=0.4, random_state=213)\n",
    "\n",
    "# Preprocesing\n",
    "X_train = [Preprocesing(text) for text in X_train]\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62623014",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tf_idf(data, max_t = 0.2, min_t = 0.01, only_short = 'True'):\n",
    "    # obliczamy tf_idf i usuwamy słowa które występują w mniej niż 1% tesktów i częściej niż w 20%\n",
    "    tf_idf = [\" \".join(text) for text in data]\n",
    "    vectorizer = TfidfVectorizer(strip_accents='unicode', stop_words='english', max_df = max_t,min_df=min_t)\n",
    "    tf_idf = vectorizer.fit_transform(tf_idf)\n",
    "    tf_idf = pd.DataFrame(tf_idf.toarray(), columns = vectorizer.get_feature_names_out())\n",
    "    return tf_idf, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df16ec1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_k = 6\n",
    "X, vectorizer = Tf_idf(X_train, 1/true_k, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6143582a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7855108d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# Create an NMF instance: model\n",
    "model = NMF(n_components = 20)\n",
    "model.fit(X)\n",
    "# Transform the articles: nmf_features\n",
    "nmf_features = model.transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751de6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "km=KMeans(n_clusters = true_k)\n",
    "km.fit(nmf_features)\n",
    "# Calculate the cluster labels: labels\n",
    "labels = km.predict(nmf_features)\n",
    "\n",
    "# Create a DataFrame aligning labels and titles: df\n",
    "df = pd.DataFrame({'label': labels, 'category': y_train})\n",
    "pd.crosstab(df['label'], df['category']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2897b9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=50)\n",
    "\n",
    "# Create a KMeans instance: kmeans\n",
    "kmeans = KMeans(n_clusters = 6)\n",
    "\n",
    "# Create a pipeline: pipeline\n",
    "pipeline = make_pipeline(svd,kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e5126d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X)\n",
    "# Calculate the cluster labels: labels\n",
    "labels = pipeline.predict(X)\n",
    "\n",
    "# Create a DataFrame aligning labels and titles: df\n",
    "df = pd.DataFrame({'label': labels, 'category': y_train})\n",
    "pd.crosstab(df['label'], df['category']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d8fb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "km=KMeans(n_clusters = 6)\n",
    "km.fit(X)\n",
    "# Calculate the cluster labels: labels\n",
    "labels = km.predict(X)\n",
    "\n",
    "# Create a DataFrame aligning labels and titles: df\n",
    "df = pd.DataFrame({'label': labels, 'category': y_train})\n",
    "pd.crosstab(df['label'], df['category']).T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9868dfe8",
   "metadata": {},
   "source": [
    "## Jeszcze cieżko stwierdzić co działa najlepiej więc odpalimy na samym tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93c23c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "clustering = DBSCAN().fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5baa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,8)).clf()\n",
    "sns.histplot(clustering.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fb3f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "km = KMeans(n_clusters =  true_k, random_state = 0)\n",
    "km.fit(X)\n",
    "labels = km.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fc87c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,8)).clf()\n",
    "sns.histplot(labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a6cc1cb",
   "metadata": {},
   "source": [
    "## Najważniejsze słowa wokół których są środki klastrów umiejscowione. Metaforycznie bo to nadal kilkaset wymiarów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea718dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = km.cluster_centers_.argsort()[:, ::-1] ## Indices of largest centroids' entries in descending order\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i, end='')\n",
    "    for ind in centroids[i, :10]:\n",
    "        print(' %s' % terms[ind], end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc6f161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A w praktyce wygląda to tak:\n",
    "def count_clustering_scores(X, cluster_num, model, score_fun):\n",
    "    # Napiszmy tę funkcje tak ogólnie, jak to możliwe. \n",
    "    # Zwróćcie uwagę na przekazanie obiektów typu callable: model i score_fun.\n",
    "    if isinstance(cluster_num, int):\n",
    "        cluster_num_iter = [cluster_num]\n",
    "    else:\n",
    "        cluster_num_iter = cluster_num\n",
    "        \n",
    "    scores = []    \n",
    "    for k in cluster_num_iter:\n",
    "        model_instance = model(n_clusters=k)\n",
    "        labels = model_instance.fit_predict(X)\n",
    "        wcss = score_fun(X, labels)\n",
    "        scores.append(wcss)\n",
    "    \n",
    "    if isinstance(cluster_num, int):\n",
    "        return scores[0]\n",
    "    else:\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18c32d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_num_seq = range(2, 60) # Niektóre metryki nie działają gdy mamy tylko jeden klaster\n",
    "silhouette_vec = count_clustering_scores(X, cluster_num_seq, KMeans, silhouette_score)\n",
    "plt.plot(cluster_num_seq, silhouette_vec, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Silhouette score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8240452a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_wcss_scores(X, k_max):\n",
    "    #  WCSS = within-cluster sum of squares\n",
    "    scores = []\n",
    "    for k in range(1, k_max+1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "        kmeans.fit(X)\n",
    "        wcss = kmeans.score(X) * -1 # score returns -WCSS\n",
    "        scores.append(wcss)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6709d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wcss_vec = count_wcss_scores(X, 60)\n",
    "x_ticks = list(range(1, len(wcss_vec) + 1))\n",
    "plt.plot(x_ticks, wcss_vec, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Within-cluster sum of squares')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
