{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daed4a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#THERE ARE TWO SEPARATE TARGET COLUMNS TO PREDICT, EITHER IN DATASET 1 AND 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29cee7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ba29b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import datasets\n",
    "df1 = pd.read_csv(\"../data/dataset_1.csv\")\n",
    "df2 = pd.read_csv(\"../data/dataset_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "392b9316",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 301)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>var_9</th>\n",
       "      <th>var_10</th>\n",
       "      <th>...</th>\n",
       "      <th>var_292</th>\n",
       "      <th>var_293</th>\n",
       "      <th>var_294</th>\n",
       "      <th>var_295</th>\n",
       "      <th>var_296</th>\n",
       "      <th>var_297</th>\n",
       "      <th>var_298</th>\n",
       "      <th>var_299</th>\n",
       "      <th>var_300</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.88</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>67772.7216</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   var_1  var_2  var_3  var_4  var_5  var_6  var_7  var_8  var_9  var_10  ...  \\\n",
       "0      0      0    0.0   0.00    0.0      0      0      0      0       0  ...   \n",
       "1      0      0    0.0   3.00    0.0      0      0      0      0       0  ...   \n",
       "2      0      0    0.0   5.88    0.0      0      0      0      0       0  ...   \n",
       "3      0      0    0.0  14.10    0.0      0      0      0      0       0  ...   \n",
       "4      0      0    0.0   5.76    0.0      0      0      0      0       0  ...   \n",
       "\n",
       "   var_292  var_293  var_294  var_295  var_296  var_297  var_298  var_299  \\\n",
       "0      0.0        0        0        0        0        0        0      0.0   \n",
       "1      0.0        0        0        0        0        0        0      0.0   \n",
       "2      0.0        0        0        3        0        0        0      0.0   \n",
       "3      0.0        0        0        0        0        0        0      0.0   \n",
       "4      0.0        0        0        0        0        0        0      0.0   \n",
       "\n",
       "      var_300  target  \n",
       "0      0.0000       0  \n",
       "1      0.0000       0  \n",
       "2  67772.7216       0  \n",
       "3      0.0000       0  \n",
       "4      0.0000       0  \n",
       "\n",
       "[5 rows x 301 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset 1 info\n",
    "print(df1.shape)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7837493b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target contains tar\n"
     ]
    }
   ],
   "source": [
    "substr = \"tar\"\n",
    "for s in df2.columns:\n",
    "    if substr in s:\n",
    "        print(f\"{s} contains {substr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b209116",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# dataset 2 info\n",
    "print(df2.shape)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823cf1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.add_suffix('_feat_1')\n",
    "df2 = df2.add_suffix('_feat_2')\n",
    "print(df1.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49a5ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.concat([df1, df2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce290dbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(merged_df.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f574daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split merged dataset into (train/test, validate) = (80%, 20%)\n",
    "merged_df, valMerged = np.split(merged_df.sample(frac=1, random_state=42), [int(.8*len(merged_df))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c53f144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting validation dataset to csv\n",
    "valMerged.to_csv('valData.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa17e6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# showing dublicate columns in data frame\n",
    "from fast_ml.utilities import display_all\n",
    "from fast_ml.feature_selection import get_duplicate_features\n",
    "\n",
    "# reset index after merge\n",
    "merged_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# checks weather there are duplicate columns names\n",
    "duplicates = list(set([x for x in list(merged_df.columns) if list(merged_df.columns).count(x) > 1]))\n",
    "print(duplicates)\n",
    "\n",
    "duplicate_features = get_duplicate_features(merged_df)\n",
    "duplicate_features.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c82c85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# flaten dataframe with duplicate columns to list\n",
    "duplicate_features_list = duplicate_features.query(\"Desc=='Duplicate Values'\")['feature2'].to_list()\n",
    "print(duplicate_features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ebd5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drops duplicate columns and shows difference to starting df\n",
    "merged_df_clean = merged_df.copy(deep=True)\n",
    "merged_df_clean.drop(columns = duplicate_features_list, inplace=True)\n",
    "print('Shape of Dataset before dropping the duplicate values features: ', merged_df.shape)\n",
    "print('Shape of Dataset after dropping the duplicate values features: ', merged_df_clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfd1481",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from fast_ml.feature_selection import get_constant_features\n",
    "\n",
    "constant_features = get_constant_features(merged_df_clean)\n",
    "constant_features.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afbe73e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"first row data = \\n\", constant_features.iloc[0], \"\\n number of rows = \\n\", len(constant_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e50abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "constant_features = constant_features.loc[(constant_features.Perc > 0.99)]\n",
    "constant_features.head(10)\n",
    "print(constant_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310c73de",
   "metadata": {},
   "outputs": [],
   "source": [
    "constant_features_list = constant_features[\"Var\"].to_list()\n",
    "print(constant_features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3963f17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Dataset shape before dropping constant and quasi constant features: \", merged_df_clean.shape)\n",
    "merged_df_clean.drop(columns = constant_features_list, inplace=True)\n",
    "print(\"Dataset shape after dropping constant and quasi constant features: \", merged_df_clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9e422c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# double check for other constant features\n",
    "quasi_constant_features = get_constant_features(merged_df_clean, threshold=0.99, dropna=False)\n",
    "quasi_constant_features.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740344a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "cor_matrix = merged_df_clean.corr().abs()\n",
    "print(cor_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7e93e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# should be graph with corellation matrix but too big data\n",
    "# import seaborn as sb\n",
    "# sb.heatmap(cor_matrix, cmap=\"Blues\", annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cae7a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_tri = cor_matrix.where(np.triu(np.ones(cor_matrix.shape), k=1).astype(np.bool))\n",
    "print(upper_tri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51fe8f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cols_to_drop = [column for column in upper_tri.columns if any(upper_tri[column]>0.95)]\n",
    "print(cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488c3b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping columns with high corellation\n",
    "merged_df_clean.drop(columns = list(cols_to_drop), axis=1, inplace=True)\n",
    "print(merged_df_clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9134ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_clean.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22618d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns with correlation less than 0.001, would we drop this?\n",
    "\n",
    "cols_to_drop = [column for column in upper_tri.columns if any(upper_tri[column]<0.001)]\n",
    "print(len(cols_to_drop))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26aa3a1",
   "metadata": {},
   "source": [
    "Description:\n",
    " - For each column, it first computes the Z-score of each value in the column, relative to the column mean and standard deviation.\n",
    "\n",
    "- It then takes the absolute Z-score because the direction does not matter, only if it is below the threshold.\n",
    "\n",
    "- all(axis=1) ensures that for each row, all column satisfy the constraint.\n",
    "\n",
    "- Finally, the result of this condition is used to index the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6416f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# description above\n",
    "from scipy import stats\n",
    "\n",
    "merged_df_clean[(np.abs(stats.zscore(merged_df_clean)) < 3).all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3ee0af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(merged_df_clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9b04da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merged_df_clean.loc[:,['target_feat_1', 'target_feat_2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d975c5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "substr = \"tar\"\n",
    "for s in merged_df_clean.columns:\n",
    "    if substr in s:\n",
    "        print(f\"{s} contains {substr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11919ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting preprocessed dataset to csv\n",
    "merged_df_clean.to_csv('cleanData.csv', encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
